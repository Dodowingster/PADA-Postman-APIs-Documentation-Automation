{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML file has been generated: index.html\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "# Function to extract `check` statements from route files\n",
    "def extract_checks_from_routes(folder_path):\n",
    "    route_data = {}\n",
    "    route_regex = re.compile(r\"routes\\.(?:get|post|put|delete|patch)\\(['\\\"](.*?)['\\\"],\\s*\\[(.*?)\\]\", re.DOTALL)\n",
    "    check_regex = re.compile(r\"check\\(['\\\"](.*?)['\\\"].*?\\)\\.(is\\w+)\")\n",
    "    body_regex = re.compile(r\"body\\(['\\\"](.*?)['\\\"].*?\\)\\.(is\\w+)\")\n",
    "    query_regex = re.compile(r\"query\\(['\\\"](.*?)['\\\"].*?\\)\\.(is\\w+)\")\n",
    "    param_regex = re.compile(r\"param\\(['\\\"](.*?)['\\\"].*?\\)\\.(is\\w+)\")\n",
    "    cookie_regex = re.compile(r\"cookie\\(['\\\"](.*?)['\\\"].*?\\)\\.(is\\w+)\")\n",
    "    header_regex = re.compile(r\"header\\(['\\\"](.*?)['\\\"].*?\\)\\.(is\\w+)\")\n",
    "    custom_regex = re.compile(r\"custom\\(['\\\"](.*?)['\\\"].*?\\)\\.(is\\w+)\")\n",
    "\n",
    "    for file_path in Path(folder_path).rglob('index.js'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            for route_match in route_regex.finditer(content):\n",
    "                route_path = route_match.group(1)\n",
    "                checks_block = route_match.group(2)\n",
    "                \n",
    "                checks = []\n",
    "                regex_pairs = [\n",
    "                    (check_regex, 'check'),\n",
    "                    (body_regex, 'body'),\n",
    "                    (query_regex, 'query'),\n",
    "                    (param_regex, 'param'),\n",
    "                    (cookie_regex, 'cookie'),\n",
    "                    (header_regex, 'header'),\n",
    "                    (custom_regex, 'custom')\n",
    "                ]\n",
    "\n",
    "                for regex, source in regex_pairs:\n",
    "                    for match in regex.finditer(checks_block):\n",
    "                        param = match.group(1)\n",
    "                        data_type = match.group(2)\n",
    "                        checks.append({\n",
    "                            'param': param, \n",
    "                            'type': data_type,\n",
    "                            'source': source\n",
    "                        })\n",
    "                \n",
    "                route_data[route_path] = checks\n",
    "    return route_data\n",
    "\n",
    "\n",
    "# Function to replace markdown links with HTML links\n",
    "def replace_markdown_links_exclude_tab_content(text):\n",
    "    # Pattern to match <div class=\"tab-content\">...</div>\n",
    "    tab_content_pattern = r'<div class=\"tab-content\">.*?</div>'\n",
    "    # Extract tab-content blocks\n",
    "    tab_content_blocks = re.findall(tab_content_pattern, text, flags=re.DOTALL)\n",
    "\n",
    "    # Temporarily replace <div class=\"tab-content\"> blocks with placeholders\n",
    "    placeholders = {}\n",
    "    for i, block in enumerate(tab_content_blocks):\n",
    "        placeholder = f\"__TAB_CONTENT_{i}__\"\n",
    "        placeholders[placeholder] = block\n",
    "        text = text.replace(block, placeholder)\n",
    "\n",
    "    # Replace markdown links outside of <div class=\"tab-content\">\n",
    "    markdown_pattern = r'\\[([^\\]]+)]\\((https?:\\/\\/[^\\)]+)\\)'\n",
    "    text = re.sub(markdown_pattern, r'<a href=\"\\2\" target=\"_blank\">\\1</a>', text)\n",
    "\n",
    "    # Restore <div class=\"tab-content\"> blocks\n",
    "    for placeholder, block in placeholders.items():\n",
    "        text = text.replace(placeholder, block)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess JSON data to replace markdown links\n",
    "def preprocess_data(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {key: preprocess_data(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [preprocess_data(item) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        return replace_markdown_links_exclude_tab_content(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Function to find parameters for a given URL\n",
    "def find_parameters(url, req_params):\n",
    "    for route, params in req_params.items():\n",
    "        if route in url:\n",
    "            return params\n",
    "    return []\n",
    "\n",
    "# Main function to integrate everything\n",
    "def main():\n",
    "    # Paths\n",
    "    routes_folder_path = \"routes\"  # Replace with your routes folder path\n",
    "    postman_input_path = \"json files/input.json\"  # Postman export file\n",
    "    output_html_path = \"index.html\"  # Output HTML file\n",
    "    template_path = \"templates/template.html\"  # Template file\n",
    "\n",
    "    # Step 1: Extract route checks and parameters\n",
    "    req_params = extract_checks_from_routes(routes_folder_path)\n",
    "\n",
    "    # Step 2: Load Postman export JSON\n",
    "    with open(postman_input_path, 'r', encoding='utf-8') as json_file:\n",
    "        postman_data = json.load(json_file)\n",
    "    postman_data = preprocess_data(postman_data)\n",
    "\n",
    "    # Step 3: Map parameters to Postman requests\n",
    "    # Inside the main function, update the request processing section:\n",
    "    for item in postman_data['item']:\n",
    "        for sub_item in item.get('item', []):\n",
    "            # Initialize empty request structure if not present\n",
    "            if 'request' not in sub_item:\n",
    "                sub_item['request'] = {}\n",
    "            \n",
    "            # Ensure all required request properties exist\n",
    "            if not isinstance(sub_item['request'], dict):\n",
    "                sub_item['request'] = {}\n",
    "            \n",
    "            # Set default values for required properties\n",
    "            sub_item['request'].setdefault('method', 'GET')\n",
    "            sub_item['request'].setdefault('url', {'raw': ''})\n",
    "            sub_item['request'].setdefault('body', {'raw': ''})\n",
    "            \n",
    "            # Initialize parameters\n",
    "            sub_item['parameters'] = []\n",
    "            \n",
    "            # Process URL if it exists and has raw property\n",
    "            if 'url' in sub_item['request'] and isinstance(sub_item['request']['url'], dict):\n",
    "                raw_url = sub_item['request']['url'].get('raw', '')\n",
    "                if raw_url:\n",
    "                    sub_item['parameters'] = find_parameters(raw_url, req_params)\n",
    "                \n",
    "        for idx, sub_item in enumerate(item['item']):\n",
    "            sub_item['_index'] = idx\n",
    "\n",
    "    # Step 4: Render HTML using Jinja2\n",
    "    env = Environment(loader=FileSystemLoader('.'))\n",
    "    template = env.get_template(template_path)\n",
    "    output = template.render(data=postman_data)\n",
    "\n",
    "    # Step 5: Write output to HTML\n",
    "    with open(output_html_path, 'w', encoding='utf-8') as html_file:\n",
    "        html_file.write(output)\n",
    "\n",
    "    print(f\"HTML file has been generated: {output_html_path}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file 'OG Node.xlsx' generated successfully!\n",
      "Columns auto-expanded in 'OG Node.xlsx'!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def extract_requests(items, parent_name=\"\"):\n",
    "    requests_data = []\n",
    "    for item in items:\n",
    "        folder_name = f\"{parent_name}/{item['name']}\" if parent_name else item['name']\n",
    "        if \"item\" in item:  # If the item contains sub-items (folders)\n",
    "            requests_data.extend(extract_requests(item[\"item\"], folder_name))\n",
    "        else:\n",
    "            request = item.get(\"request\", {})\n",
    "            body = request.get(\"body\", {}).get(\"raw\", \"\")\n",
    "            \n",
    "            requests_data.append({\n",
    "                \"Folder\": parent_name,\n",
    "                \"Name\": item.get(\"name\", \"\"),\n",
    "                \"Method\": request.get(\"method\", \"\"),\n",
    "                \"URL\": request.get(\"url\", {}).get(\"raw\", \"\"),\n",
    "                \"Body\": body\n",
    "            })\n",
    "    return requests_data\n",
    "\n",
    "def parse_postman_collection(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return extract_requests(data.get(\"item\", [])), data.get(\"info\", {}).get(\"name\", \"default_collection\")\n",
    "\n",
    "def save_to_excel(data, output_file):\n",
    "    # Group data by 'Folder'\n",
    "    grouped_data = {}\n",
    "    for request in data:\n",
    "        folder = request['Folder']\n",
    "        if folder not in grouped_data:\n",
    "            grouped_data[folder] = []\n",
    "        grouped_data[folder].append(request)\n",
    "    \n",
    "    # Create a new Excel writer\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        for folder, folder_data in grouped_data.items():\n",
    "            # Create a DataFrame for each folder and save to a separate sheet\n",
    "            df = pd.DataFrame(folder_data)\n",
    "            df.to_excel(writer, sheet_name=folder, index=False)\n",
    "        \n",
    "        print(f\"Excel file '{output_file}' generated successfully!\")\n",
    "\n",
    "    # Open the workbook and adjust column widths\n",
    "    wb = load_workbook(output_file)\n",
    "    for sheet in wb.sheetnames:\n",
    "        sheet_obj = wb[sheet]\n",
    "        for col in sheet_obj.columns:\n",
    "            max_length = 0\n",
    "            column = col[0].column_letter  # Get the column name\n",
    "            for cell in col:\n",
    "                try:\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(cell.value)\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = (max_length + 2)  # Adding a bit of padding\n",
    "            sheet_obj.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "    wb.save(output_file)\n",
    "    print(f\"Columns auto-expanded in '{output_file}'!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json = \"json files/input.json\"  # Change to your Postman collection file\n",
    "    parsed_data, collection_name = parse_postman_collection(input_json)\n",
    "    \n",
    "    output_excel = f\"{collection_name}.xlsx\"\n",
    "    \n",
    "    save_to_excel(parsed_data, output_excel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
